{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f3b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login as hf_login\n",
    "load_dotenv()\n",
    "hf_login(os.environ['HF_TOKEN'])\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,model_name):\n",
    "        self.model_name=model_name\n",
    "        # Load tokenizer with correct padding configuration\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "\n",
    "        # Load model with optimizations\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if device in [\"cuda\", \"mps\"] else torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        self.model.to(device)\n",
    "    def generate(self,prompt,max_new_tokens=100,temperature=.1):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Move tensors to the right device\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs.get(\"attention_mask\", None)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Clear CUDA cache if using GPU\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        # Generate with optimal parameters for the model\n",
    "        with torch.no_grad():\n",
    "            generate_kwargs = {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"do_sample\": True,\n",
    "                \"top_p\": 0.95,\n",
    "                \"top_k\": 50,\n",
    "                \"repetition_penalty\": 1.4,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"use_cache\": True\n",
    "            }\n",
    "        if attention_mask is not None:\n",
    "            generate_kwargs[\"attention_mask\"] = attention_mask\n",
    "            \n",
    "        output = self.model.generate(input_ids, **generate_kwargs)\n",
    "\n",
    "        # Extract the model's response\n",
    "        input_length = input_ids.shape[1]\n",
    "        response = self.tokenizer.decode(output[0, input_length:], skip_special_tokens=True).strip()\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e683ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dd3a1b1bf244218818f24c0978089a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m=Model(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907e168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4ed84ae7240f2a2574ef1c93602a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35096c8e536540f29cd7a53592d3ba14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797d3f2aa2c144dd95dd6a54a4a43580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98786a670d740079a4aac7c5074280d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426059404b074b79ae47be1d804c00a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fbd370898b43ec83993557e993667e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f729f7b9fb8e42d6a9eb9d07fa34971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=m.generate(\"Write a poem about New York City\",max_new_tokens=100,temperature=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baec4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Write a poem about New York City\"\n",
    "temperature=.1\n",
    "max_new_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e98a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8699f98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The city that never sleeps, they say,\\nA place where dreams are made and lost each day.\\nSkyscrapers pierce the urban sky,\\nConcrete jungle stretching far and high.\\n\\nIn Times Square's bright lights I roam,\\nWhere billboards flash with endless tone.\\nBroadway beckons to my soul,\\nTo dance in theaters' grand control.\\n\\nCentral Park offers peaceful nest,\\nGreen oasis amidst steel unrest.\\nBrooklyn Bridge spans waters wide,\\nConnecting boroughs side by side.\\n\\nFrom Harlem jazz\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb898de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   8144,    264,  33894,    922,   1561,   4356,   4409,    198,\n",
       "            791,   3363,    430,   2646,  72490,     11,    814,   2019,    345,\n",
       "             32,   2035,   1405,  19226,    527,   1903,    323,   5675,   1855,\n",
       "           1938,    627,  19847,   1065,  99821,    388,  22710,    346,    279,\n",
       "          16036,  13180,    345,  84694,  45520,  42949,   3117,    323,   1579,\n",
       "            382,    644,   8691,  15992,    596,  10107,  13001,    358,  76067,\n",
       "            345,   9241,   4121,  19826,   8381,    449,  26762,  16630,    627,\n",
       "          69424,   3195,  92186,   2439,    311,    856,  13836,    345,   1271,\n",
       "          15612,    304,  44866,      6,   6800,   2585,    382,  44503,   5657,\n",
       "           6209,  26733,  23634,    345,  20147,  86810,  65904,   9699,  59322,\n",
       "            627,  27368,  83287,  20467,  45395,  21160,   7029,    345,  64024,\n",
       "          66841,     82,   3185,    555,   3185,    382,   3915,  83852,  34997]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9dbf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
