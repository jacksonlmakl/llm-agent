{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5272a13e",
   "metadata": {},
   "source": [
    "# Import Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "040f4664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackson.makl@dataiku.com/Documents/llm-agent/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "2025-05-14 23:26:21,685 - llm - WARNING - HF_TOKEN not found in environment variables. Running without authentication.\n"
     ]
    }
   ],
   "source": [
    "from llm import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1532812",
   "metadata": {},
   "source": [
    "# Initialize a Model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a62482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 23:26:21,693 - llm - INFO - Initializing model: meta-llama/Llama-3.2-3B-Instruct\n",
      "2025-05-14 23:26:21,714 - llm - INFO - Using device: mps\n",
      "2025-05-14 23:26:22,123 - llm - INFO - Pad token not found, using EOS token as pad token\n",
      "2025-05-14 23:26:22,123 - llm - INFO - Tokenizer loaded: PreTrainedTokenizerFast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0aa0f5e93c54514a8fff2f1f866c357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 23:26:40,849 - llm - INFO - Model loaded: LlamaForCausalLM\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model(\"meta-llama/Llama-3.2-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac79e1b",
   "metadata": {},
   "source": [
    "# Generate a response from the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4593fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackson.makl@dataiku.com/Documents/llm-agent/env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:698: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `0.2` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A delightful topic for a philosophical debate.\n",
      "\n",
      "The Trolley Problem is a classic thought experiment that raises questions about moral decision-making, particularly when it comes to ethics and consequentialism. The Utilitarian Perspective views the situation as follows:\n",
      "\n",
      "Imagine you're standing near a railway track with one trolley (or tram) headed towards five people who will certainly be killed if left unattended. However, you notice that there's a lever nearby that can divert the trolley onto a side track where only one person is standing â€“ also destined to die. If you pull the lever, you'll save four lives but actively cause the death of one person.\n",
      "\n",
      "From a Utilitarian standpoint, this scenario presents us with a difficult choice between two options:\n",
      "\n",
      "**Option A:** Do nothing, allowing the troller to continue on its course and killing all five passengers.\n",
      "**Option B:** Pull the lever to divert the train onto the side track, saving four lives at the cost of one life.\n",
      "\n",
      "Utilitarians would argue that Option B is the morally justifiable choice because it results in greater overall happiness or well-being. By pulling the lever and diverting the terry into the less populated track, we maximize the number of lives saved while minimizing harm. In this case, the\n"
     ]
    }
   ],
   "source": [
    "response = model.generate(\n",
    "    prompt=\"Explain to me the utilitarian perspective on the trolly problem.\",\n",
    "    max_new_tokens= 250, \n",
    "    temperature= 0.7, \n",
    "    typical_p = .9,\n",
    "    length_penalty=.2,\n",
    "    # top_p = .95,\n",
    "    # top_k= 50,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.2,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant. You are skilled in philosophy, and love to play devils advocate.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f15cd411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAD\n"
     ]
    }
   ],
   "source": [
    "emotional_response = model.generate(\n",
    "    prompt=\"I fucking hate everything im going to cry\",\n",
    "    max_new_tokens=5,        # Reduced: Only need 1 token for \"HAPPY\" or \"SAD\"\n",
    "    temperature=0.05,        # Reduced: More deterministic output\n",
    "    top_p=0.9,               # Added: Focus on highest probability tokens\n",
    "    top_k=5,                 # Added: Restrict to only the most likely options\n",
    "    repetition_penalty=1.0,  # Removed: Not needed for single-word output\n",
    "    no_repeat_ngram_size=0,  # Removed: Not needed for single-word output\n",
    "    # Removed: typical_p and length_penalty (unnecessary complexity)\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a precise sentiment analysis classifier. Your task is to categorize text as either 'HAPPY' or 'SAD' based solely on the emotional content. Respond with ONLY the word 'HAPPY' or 'SAD' in uppercase - no other text. Anything else will be considered a failure.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"I love my life so much\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"HAPPY\"},\n",
    "        {\"role\": \"user\", \"content\": \"I hate everything about myself\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SAD\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just got a promotion at work!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"HAPPY\"},\n",
    "        {\"role\": \"user\", \"content\": \"I failed my exam despite studying for weeks\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SAD\"},\n",
    "    ]\n",
    ")\n",
    "print(emotional_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac993e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPY\n"
     ]
    }
   ],
   "source": [
    "emotional_response = model.generate(\n",
    "    prompt=\"I just had the best meal at Maomao in Brooklyn New York. It is the coolest Thai place in the city.\",\n",
    "    max_new_tokens=5,        # Reduced: Only need 1 token for \"HAPPY\" or \"SAD\"\n",
    "    temperature=0.05,        # Reduced: More deterministic output\n",
    "    top_p=0.9,               # Added: Focus on highest probability tokens\n",
    "    top_k=5,                 # Added: Restrict to only the most likely options\n",
    "    repetition_penalty=1.0,  # Removed: Not needed for single-word output\n",
    "    no_repeat_ngram_size=0,  # Removed: Not needed for single-word output\n",
    "    # Removed: typical_p and length_penalty (unnecessary complexity)\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a precise sentiment analysis classifier. Your task is to categorize text as either 'HAPPY' or 'SAD' based solely on the emotional content. Respond with ONLY the word 'HAPPY' or 'SAD' in uppercase - no other text. Anything else will be considered a failure.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"I love my life so much\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"HAPPY\"},\n",
    "        {\"role\": \"user\", \"content\": \"I hate everything about myself\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SAD\"},\n",
    "        {\"role\": \"user\", \"content\": \"Just got a promotion at work!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"HAPPY\"},\n",
    "        {\"role\": \"user\", \"content\": \"I failed my exam despite studying for weeks\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SAD\"},\n",
    "    ]\n",
    ")\n",
    "print(emotional_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8bf7",
   "metadata": {},
   "source": [
    "# Query internal messages Duck Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bf1836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>role</th>\n",
       "      <th>content</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70522a6e1d4fd3ee12e0dbb52841f4dc2227f1253f0eb9...</td>\n",
       "      <td>150b88c4ad8040f152b3a74607951f9c4f3ac4e2f5672e...</td>\n",
       "      <td>user</td>\n",
       "      <td>Explain to me the utilitarian perspective on t...</td>\n",
       "      <td>2025-05-14 23:27:05.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0d12d0bea77f5e8f048484339c903fe239a51e10642bc5...</td>\n",
       "      <td>150b88c4ad8040f152b3a74607951f9c4f3ac4e2f5672e...</td>\n",
       "      <td>assistant</td>\n",
       "      <td>A delightful topic for a philosophical debate....</td>\n",
       "      <td>2025-05-14 23:27:05.633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          message_id  \\\n",
       "0  70522a6e1d4fd3ee12e0dbb52841f4dc2227f1253f0eb9...   \n",
       "1  0d12d0bea77f5e8f048484339c903fe239a51e10642bc5...   \n",
       "\n",
       "                                          session_id       role  \\\n",
       "0  150b88c4ad8040f152b3a74607951f9c4f3ac4e2f5672e...       user   \n",
       "1  150b88c4ad8040f152b3a74607951f9c4f3ac4e2f5672e...  assistant   \n",
       "\n",
       "                                             content            created_date  \n",
       "0  Explain to me the utilitarian perspective on t... 2025-05-14 23:27:05.632  \n",
       "1  A delightful topic for a philosophical debate.... 2025-05-14 23:27:05.633  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.db_conn.execute(\"\"\"\n",
    "SELECT * FROM messages\n",
    "\"\"\")\n",
    "df=model.db_conn.fetch_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e5abd",
   "metadata": {},
   "source": [
    "# Create Vector Database & add documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0872a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_db import VectorDB\n",
    "\n",
    "# Create and clear the database\n",
    "db = VectorDB()\n",
    "db.clear_db()\n",
    "\n",
    "# Add documents\n",
    "db.add_document(\"This is a document about artificial intelligence.\")\n",
    "db.add_document(\"Vector databases are useful for similarity search.\")\n",
    "\n",
    "# Add multiple documents at once\n",
    "db.add_documents([\n",
    "    \"Python is a popular programming language.\",\n",
    "    \"Embedding models convert text to vectors.\",\n",
    "    \"FAISS is a library for efficient similarity search.\"\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3b08a",
   "metadata": {},
   "source": [
    "# Perform search on vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633731fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = db.search(\"How do AI systems work specifically with FAISS?\", k=3)\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd5c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
